{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDA Database generation\n",
    "\n",
    "## Scraping the FDA website to populate a database with \n",
    "1. Drug name\n",
    "2. Drug maker\n",
    "3. Event date\n",
    "4. Event type\n",
    "5. Company Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython import embed\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sourceGetter(urlSource=\n",
    "                 'http://www.fda.gov/BiologicsBloodVaccines/DevelopmentApprovalProcess/BiologicalApprovalsbyYear/default.htm'):\n",
    "\n",
    "# This function gets the urls for each year of FDA approval for biolgics.\n",
    "# The default input is the url that includes the list of links to pages for each year since 1996.\n",
    "# Used as input for blaGetter, which finds the links to list of events in each year entry\n",
    "    urlList = []\n",
    "    with urllib.request.urlopen(urlSource) as response: #imports html from the url\n",
    "       html = response.read()\n",
    "    soup = BeautifulSoup(html,'lxml') #parses the html into a useful tree structure\n",
    "    \n",
    "    for link in soup.find(id='section-menu').find_all('a'): #iterates through the links to year pages and extracts link\n",
    "        if link.get('class') == ['list-group-item']:\n",
    "            urlList.append('http://www.fda.gov' + \n",
    "                           link.get('href'))\n",
    "            \n",
    "    return(urlList)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blaGetter(blaSource = \n",
    "              'http://www.fda.gov/BiologicsBloodVaccines/DevelopmentApprovalProcess/BiologicalApprovalsbyYear/ucm482392.htm',\n",
    "             n = 3):\n",
    "# This function takes the list of urls output by sourceGetter and extracts links for the three\n",
    "# types of FDA approval events for biologics: BLA, supplement, and NDA.\n",
    "    blaList = []\n",
    "    with urllib.request.urlopen(blaSource) as response: # imports html from url\n",
    "        html = response.read()\n",
    "    soup = BeautifulSoup(html,'lxml') # parse and construct the tree\n",
    "    soup = soup.find('article') # navigating through the tree\n",
    "    soup.find('header').extract()\n",
    "    soup = soup.find('ul')\n",
    "    for link in soup.find_all('li'): # generator object that returns links for BLA, supp., and NDA.\n",
    "        L = link.find('a').get('href')\n",
    "        yield('http://www.fda.gov' + L)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uL = sourceGetter() #compile list of year pages\n",
    "uL\n",
    "bL_BLAapproval = [] \n",
    "bL_BLAsupplement = []\n",
    "bL_Bnda = []\n",
    "for u in uL: #iterate through years, extracting BLA, supp., and NDA links\n",
    "    gen = blaGetter(u)\n",
    "    for i in range(0,3): #splitting the links returned by blaGetter into three different lists\n",
    "        try: #error catching to deal with years that don't have all three regulation pages\n",
    "            B = next(gen)\n",
    "            if i == 0:bL_BLAapproval.append(B)\n",
    "            elif i == 1:bL_BLAsupplement.append(B)\n",
    "            elif i == 2:bL_Bnda.append(B)\n",
    "        except StopIteration: pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataFinder(entry,n,eventType):\n",
    "# This function sits inside blaEntry (below) and extract information for addition to the database\n",
    "    tds = entry.find_all('td') #navigating the tree and extracting information\n",
    "    try: drugName = entry.a.string.extract()\n",
    "    except: drugName = 'error'\n",
    "    try: STN = tds[2].string\n",
    "    except: STN = 'error'\n",
    "    try: companyName = tds[3].contents[0].string\n",
    "    except: companyName = 'error'\n",
    "    try: eventDate = tds[4].string\n",
    "    except:eventDate = 'error' \n",
    "    noteworthFDA = ['NO']\n",
    "    # populate a dict object for appending to the dataframe\n",
    "    d = {'drugName':drugName, \n",
    "         'STN':STN,\n",
    "         'companyName':companyName,\n",
    "         'eventDate':eventDate,\n",
    "         'eventType':eventType\n",
    "        }\n",
    "    \n",
    "    return(pd.DataFrame(data=d)) #return the dict as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blaEntry(url,eventType):\n",
    "# Takes the url output by blaGetter, and calls dataFinder to grab values for columns in the dataframe\n",
    "# Note: eventType is passed through to dataFinder so that dataFinder can return the eventType in\n",
    "# the dataframe object.\n",
    "    df = pd.DataFrame(columns = ('drugName','STN','companyName','eventDate','eventType'))\n",
    "    with urllib.request.urlopen(url) as response: #html from url\n",
    "        html = response.read()\n",
    "    soup = BeautifulSoup(html,'lxml') #tree from html\n",
    "    try: entries = soup.tbody.find_all('tr')\n",
    "    except AttributeError: entries = soup.table.find_all('tr')\n",
    "    for entry in entries:\n",
    "        n = 0\n",
    "        dfT = dataFinder(entry,n,eventType) #temporary dataframe with dataFinder\n",
    "        dfT\n",
    "        df = pd.concat([df,dfT]) #add temporary dataframe to master\n",
    "    return(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Takes the urls from sourceGetter and blaGetter and builds the dataframe with \n",
    "# blaEntry and dataFinder\n",
    "df = pd.DataFrame(columns = ('drugName','STN','companyName','eventDate','eventType'))\n",
    "for b in bL_BLAapproval:\n",
    "    dfT = blaEntry(b,['BLAapproval'])\n",
    "    df = pd.concat([df,dfT])\n",
    "\n",
    "for b in bL_BLAsupplement:\n",
    "    dfT = blaEntry(b,['BLAsupplement'])\n",
    "    df = pd.concat([df,dfT])\n",
    "\n",
    "for b in bL_Bnda:\n",
    "    dfT = blaEntry(b,['Bnda'])\n",
    "    df = pd.concat([df,dfT])\n",
    "\n",
    "# saves df as to desktop as a csv\n",
    "df.to_csv(path_or_buf = '/Users/Jonathan/Desktop/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
